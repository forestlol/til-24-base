{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbf88a-f2fe-4f23-b59d-d609f7f329aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 07:57:50.499689: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5107 annotations.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Paths to your dataset\n",
    "image_dir = \"../../advance/images\"\n",
    "annotations_path = \"../../advance/vlm.jsonl\"\n",
    "\n",
    "# Load annotations\n",
    "annotations = []\n",
    "with open(annotations_path, 'r') as f:\n",
    "    for line in f:\n",
    "        annotations.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(annotations)} annotations.\")\n",
    "\n",
    "# Load images and corresponding annotations\n",
    "def load_data(annotations, image_dir):\n",
    "    images = []\n",
    "    captions = []\n",
    "    bboxes = []\n",
    "\n",
    "    for annotation in annotations:\n",
    "        image_path = os.path.join(image_dir, annotation['image'])\n",
    "        image = load_img(image_path, target_size=(224, 224))\n",
    "        image = img_to_array(image) / 255.0\n",
    "\n",
    "        for ann in annotation['annotations']:\n",
    "            captions.append(ann['caption'])\n",
    "            bboxes.append(ann['bbox'])\n",
    "        \n",
    "        images.append(image)\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images.\")\n",
    "    return np.array(images), captions, bboxes\n",
    "\n",
    "images, captions, bboxes = load_data(annotations, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc5b65-5b30-4bb0-8215-0681b95a5ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(captions)\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(f\"Tokenized {len(captions)} captions into sequences.\")\n",
    "\n",
    "# Normalize bounding boxes\n",
    "def normalize_bbox(bbox, image_shape):\n",
    "    height, width, _ = image_shape\n",
    "    x, y, w, h = bbox\n",
    "    return [x / width, y / height, w / width, h / height]\n",
    "\n",
    "normalized_bboxes = [normalize_bbox(bbox, images[0].shape) for bbox in bboxes]\n",
    "print(f\"Normalized {len(normalized_bboxes)} bounding boxes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce364c63-4d7a-4069-adb6-6676fb2ba0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Concatenate, Flatten\n",
    "\n",
    "# Image feature extractor\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "x = Flatten()(base_model.output)\n",
    "image_features = Dense(256, activation='relu')(x)\n",
    "\n",
    "# Caption processor\n",
    "caption_input = Input(shape=(None,))\n",
    "embedding = Embedding(input_dim=10000, output_dim=256, mask_zero=True)(caption_input)\n",
    "lstm_out = LSTM(256)(embedding)\n",
    "\n",
    "# Bounding box predictor\n",
    "combined = Concatenate()([image_features, lstm_out])\n",
    "bbox_output = Dense(4, activation='sigmoid')(combined)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[image_input, caption_input], outputs=bbox_output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a7eb6-105d-4f21-b1e8-dbce2ab8fabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the inputs\n",
    "def prepare_inputs(images, sequences, normalized_bboxes, max_seq_length):\n",
    "    images_input = np.array(images)\n",
    "    sequences_input = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "    bboxes_output = np.array(normalized_bboxes)\n",
    "    \n",
    "    return images_input, sequences_input, bboxes_output\n",
    "\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "images_input, sequences_input, bboxes_output = prepare_inputs(images, sequences, normalized_bboxes, max_seq_length)\n",
    "\n",
    "print(f\"Prepared inputs: images_input shape = {images_input.shape}, sequences_input shape = {sequences_input.shape}, bboxes_output shape = {bboxes_output.shape}\")\n",
    "\n",
    "# Train the model\n",
    "model.fit([images_input, sequences_input], bboxes_output, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff0daa-6fd0-430b-8394-21f24604bad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Further fine-tuning can be done by adjusting learning rates, using callbacks, etc.\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=2)\n",
    "]\n",
    "\n",
    "history = model.fit([images_input, sequences_input], bboxes_output, epochs=50, batch_size=32, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "print(f\"Training completed. History: {history.history}\")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss = model.evaluate([images_input, sequences_input], bboxes_output)\n",
    "print(f\"Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507f5d1-9df3-42c7-8e2d-fe63b4d0aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Paths to your dataset\n",
    "image_dir = \"../../advance/images\"\n",
    "annotations_path = \"../../advance/vlm.jsonl\"\n",
    "\n",
    "# Load annotations\n",
    "annotations = []\n",
    "with open(annotations_path, 'r') as f:\n",
    "    for line in f:\n",
    "        annotations.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(annotations)} annotations.\")\n",
    "\n",
    "# Select one random annotation\n",
    "random_annotation = random.choice(annotations)\n",
    "image_path = os.path.join(image_dir, random_annotation['image'])\n",
    "image = load_img(image_path, target_size=(224, 224))\n",
    "image = img_to_array(image) / 255.0\n",
    "\n",
    "print(f\"Loaded random image: {random_annotation['image']}\")\n",
    "\n",
    "# Get captions and bounding boxes for the selected image\n",
    "captions = [ann['caption'] for ann in random_annotation['annotations']]\n",
    "bboxes = [ann['bbox'] for ann in random_annotation['annotations']]\n",
    "print(f\"Captions: {captions}\")\n",
    "print(f\"Bounding Boxes: {bboxes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf571f0a-f6a5-4a97-9cd0-0fe7f3621bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(captions)\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(f\"Tokenized captions: {sequences}\")\n",
    "\n",
    "# Normalize bounding boxes\n",
    "def normalize_bbox(bbox, image_shape):\n",
    "    height, width, _ = image_shape\n",
    "    x, y, w, h = bbox\n",
    "    return [x / width, y / height, w / width, h / height]\n",
    "\n",
    "normalized_bboxes = [normalize_bbox(bbox, image.shape) for bbox in bboxes]\n",
    "print(f\"Normalized bounding boxes: {normalized_bboxes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a1ff6-bfd8-4e4d-ac72-48e791180d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inputs\n",
    "images_input = np.expand_dims(image, axis=0)\n",
    "sequences_input = pad_sequences(sequences, maxlen=pad_sequences(sequences).shape[1], padding='post')\n",
    "sequences_input = np.expand_dims(sequences_input[0], axis=0)  # Make it batch-compatible\n",
    "\n",
    "print(f\"Prepared inputs: images_input shape = {images_input.shape}, sequences_input shape = {sequences_input.shape}\")\n",
    "\n",
    "# Load the model (assumes model definition and compilation from earlier)\n",
    "# model = ...  # Define and compile the model as shown earlier\n",
    "\n",
    "# Make predictions\n",
    "predicted_bboxes = model.predict([images_input, sequences_input])\n",
    "print(f\"Predicted bounding boxes: {predicted_bboxes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76d418-f36e-4648-96e7-c4d964c6d9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
